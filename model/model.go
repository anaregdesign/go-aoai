package model

import "encoding/json"

type CompletionRequest struct {
	// prompt:
	//   description: |-
	//     The prompt(s) to generate completions for, encoded as a string or array of strings.
	//     Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. Maximum allowed size of string list is 2048.
	//   oneOf:
	//     - type: string
	//       default: ''
	//       example: This is a test.
	//       nullable: true
	//     - type: array
	//       items:
	//         type: string
	//         default: ''
	//         example: This is a test.
	//         nullable: false
	//       description: Array size minimum of 1 and maximum of 2048
	Prompt []string `json:"prompt"`

	// max_tokens:
	//   description: The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0.
	//   type: integer
	//   default: 16
	//   example: 16
	//   nullable: true
	MaxTokens int `json:"max_tokens,omitempty"`

	// temperature:
	//   description: |-
	//     What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
	//     We generally recommend altering this or top_p but not both.
	//   type: number
	//   default: 1
	//   example: 1
	//   nullable: true
	Temperature float64 `json:"temperature,omitempty"`

	// top_p:
	//   description: |-
	//     An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	//     We generally recommend altering this or temperature but not both.
	//   type: number
	//   default: 1
	//   example: 1
	//   nullable: true
	TopP float64 `json:"top_p,omitempty"`

	// logit_bias:
	//   description: Defaults to null. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {"50256" &#58; -100} to prevent the <|endoftext|> token from being generated.
	//   type: object
	//   nullable: false
	LogitBias map[string]int `json:"logit_bias,omitempty"`

	// user:
	//   description: A unique identifier representing your end-user, which can help monitoring and detecting abuse
	//   type: string
	//   nullable: false
	User string `json:"user,omitempty"`

	// 'n':
	//   description: |-
	//     How many completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.
	//     Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.
	//   type: integer
	//   default: 1
	//   example: 1
	//   nullable: true
	N int `json:"n,omitempty"`

	// stream:
	//   description: 'Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.'
	//   type: boolean
	//   nullable: true
	//   default: false
	Stream bool `json:"stream,omitempty"`

	// logprobs:
	//   description: |-
	//     Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
	//     Minimum of 0 and maximum of 5 allowed.
	//   type: integer
	//   default: null
	//   nullable: true
	Logprobs int `json:"logprobs,omitempty"`

	// model:
	//   type: string
	//   example: davinci
	//   nullable: true
	//   description: ID of the model to use. You can use the Models_List operation to see all of your available models, or see our Models_Get overview for descriptions of them.
	Model string `json:"model,omitempty"`

	// suffix:
	//   type: string
	//   nullable: true
	//   description: The suffix that comes after a completion of inserted text.
	Suffix string `json:"suffix,omitempty"`

	// echo:
	//   description: Echo back the prompt in addition to the completion
	//   type: boolean
	//   default: false
	//   nullable: true
	Echo bool `json:"echo,omitempty"`

	// stop:
	//   description: Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
	//   oneOf:
	//     - type: string
	//       default: <|endoftext|>
	//       example: |+
	//
	//       nullable: true
	//     - type: array
	//       items:
	//         type: string
	//         example:
	//           - |+
	//
	//         nullable: false
	//       description: Array minimum size of 1 and maximum of 4
	Stop []string `json:"stop,omitempty"`

	// completion_config:
	//   type: string
	//   nullable: true
	CompletionConfig string `json:"completion_config,omitempty"`

	// cache_level:
	//   description: can be used to disable any server-side caching, 0=no cache, 1=prompt prefix enabled, 2=full cache
	//   type: integer
	//   nullable: true
	CacheLevel int `json:"cache_level,omitempty"`

	// presence_penalty:
	//   description: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
	//   type: number
	//   default: 0
	PresencePenalty float64 `json:"presence_penalty,omitempty"`

	// frequency_penalty:
	//   description: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	//   type: number
	//   default: 0
	FrequencyPenalty float64 `json:"frequency_penalty,omitempty"`

	// best_of:
	//   description: |-
	//     Generates best_of completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.
	//     When used with n, best_of controls the number of candidate completions and n specifies how many to return â€“ best_of must be greater than n.
	//     Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. Has maximum value of 128.
	//   type: integer
	BestOf int `json:"best_of,omitempty"`
}

type CompletionResponse struct {
	//  id:
	//    type: string
	ID string `json:"id,omitempty"`

	//  object:
	//    type: string
	Object string `json:"object,omitempty"`

	//  created:
	//    type: integer
	Created int `json:"created,omitempty"`

	//  model:
	//    type: string
	Model string `json:"model,omitempty"`

	//  choices:
	//    type: array
	//    items:
	//      type: CompletionChoice
	Choices []CompletionChoice `json:"choices,omitempty"`
}

type CompletionChoice struct {
	// text:
	//   type: string
	Text string `json:"text,omitempty"`

	// index:
	//   type: integer
	Index int `json:"index,omitempty"`

	// logprobs:
	//   type: Logprobes
	Logprobs Logprobs `json:"logprobs,omitempty"`

	// finish_reason:
	//   type: string
	FinishReason string `json:"finish_reason,omitempty"`
}

type Logprobs struct {
	// tokens:
	//   type: array
	//   items:
	//     type: string
	Tokens []string `json:"tokens,omitempty"`

	// token_logprobs:
	//   type: array
	//   items:
	//     type: number
	TokenLogprobs []float64 `json:"token_logprobs,omitempty"`

	// top_logprobs:
	//   type: array
	//   items:
	//     type: object
	//     additionalProperties:
	//       type: number
	TopLogprobs []map[string]float64 `json:"top_logprobs,omitempty"`

	// text_offset:
	//   type: array
	//   items:
	//     type: integer
	TextOffset []int `json:"text_offset,omitempty"`
}

// {
//  "input": "This is a test.",
//  "user": "string",
//  "input_type": "query",
//  "model": "string",
//  "additionalProp1": {}
//}

type EmbeddingRequest struct {
	Input      string                 `json:"input"`
	User       string                 `json:"user"`
	InputType  string                 `json:"input_type"`
	Model      string                 `json:"model"`
	Additional map[string]interface{} `json:"additionalProp1"`
}

// {
//  "object": "string",
//  "model": "string",
//  "data": [
//    {
//      "index": 0,
//      "object": "string",
//      "embedding": [
//        0
//      ]
//    }
//  ],
//  "usage": {
//    "prompt_tokens": 0,
//    "total_tokens": 0
//  }
//}

type Data struct {
	Index     int     `json:"index"`
	Object    string  `json:"object"`
	Embedding []int64 `json:"embedding"`
}

type Usage struct {
	CompletionTokens int `json:"completion_tokens"`
	PromptTokens     int `json:"prompt_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

type EmbeddingResponse struct {
	Object string `json:"object"`
	Model  string `json:"model"`
	Data   []Data `json:"data"`
	Usage  Usage  `json:"usage"`
}

// {
//  "model": "gpt-35-turbo",
//  "messages": [
//    {
//      "role": "user",
//      "content": "Hello!"
//    }
//  ]
//}

type Message struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type ChatRequest struct {
	Model    string    `json:"model"`
	Messages []Message `json:"messages"`
}

// {
//  "id": "chatcmpl-123",
//  "object": "chat.completion",
//  "created": 1677652288,
//  "choices": [
//    {
//      "index": 0,
//      "message": {
//        "role": "assistant",
//        "content": "\n\nHello there, how may I assist you today?"
//      },
//      "finish_reason": "stop"
//    }
//  ],
//  "usage": {
//    "prompt_tokens": 9,
//    "completion_tokens": 12,
//    "total_tokens": 21
//  }
//}

type ChatChoice struct {
	Index        int     `json:"index"`
	Message      Message `json:"message"`
	FinishReason string  `json:"finish_reason"`
}

type ChatResponse struct {
	ID      string       `json:"id"`
	Object  string       `json:"object"`
	Created int          `json:"created"`
	Choices []ChatChoice `json:"choices"`
	Usage   Usage        `json:"usage"`
}

// {
//  "error": {
//    "code": "string",
//    "message": "string",
//    "param": "string",
//    "type": "string"
//  }
//}

type Error struct {
	Code    string `json:"code"`
	Message string `json:"message"`
	Param   string `json:"param"`
	Type    string `json:"type"`
}

func (e *Error) Error() string {
	m, _ := json.Marshal(e)
	return string(m)
}

type ErrorResponse struct {
	Error Error `json:"error"`
}
